# %% [markdown]
# # Final Project: Building a Rainfall Prediction Classifier
# 
# ## Introduction
# In this project, we will build a machine learning model to predict whether it will rain tomorrow based on weather data. We will use various classification algorithms and evaluate their performance.

# %% [markdown]
# ## Exercise 1: Importing the Required Libraries
# Import all necessary libraries for data manipulation, visualization, and machine learning.

# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, roc_auc_score
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

# Set style for plots
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# %% [markdown]
# ## Exercise 2: Loading the Dataset
# Load the weather dataset from the CSV file.

# %%
# Load the dataset
df = pd.read_csv('weatherAUS.csv')

# Display basic information
print("Dataset Shape:", df.shape)
print("\nFirst 5 rows:")
print(df.head())
print("\nDataset Info:")
print(df.info())

# %% [markdown]
# ## Exercise 3: Initial Data Exploration
# Explore the dataset to understand its structure and contents.

# %%
# Display basic statistics
print("Basic Statistics:")
print(df.describe())

# Check for missing values
print("\nMissing Values per Column:")
missing_values = df.isnull().sum()
print(missing_values[missing_values > 0])

# Check data types
print("\nData Types:")
print(df.dtypes)

# Check target variable distribution
print("\nTarget Variable Distribution (RainTomorrow):")
print(df['RainTomorrow'].value_counts())
print("\nPercentage:")
print(df['RainTomorrow'].value_counts(normalize=True) * 100)

# %% [markdown]
# ## Exercise 4: Data Preprocessing - Handling Missing Values
# Handle missing values in the dataset.

# %%
# Create a copy of the dataframe
df_clean = df.copy()

# Separate numerical and categorical columns
numerical_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()

# Remove target variable from categorical columns
categorical_cols.remove('RainTomorrow')

# Handle missing values for numerical columns - impute with median
num_imputer = SimpleImputer(strategy='median')
df_clean[numerical_cols] = num_imputer.fit_transform(df_clean[numerical_cols])

# Handle missing values for categorical columns - impute with mode
for col in categorical_cols:
    mode_value = df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown'
    df_clean[col].fillna(mode_value, inplace=True)

# Check remaining missing values
print("Remaining Missing Values:")
print(df_clean.isnull().sum().sum())

# %% [markdown]
# ## Exercise 5: Feature Engineering
# Create new features and transform existing ones for better model performance.

# %%
# Create new features
df_clean['TempRange'] = df_clean['MaxTemp'] - df_clean['MinTemp']
df_clean['PressureChange'] = df_clean['Pressure9am'] - df_clean['Pressure3pm']
df_clean['HumidityChange'] = df_clean['Humidity3pm'] - df_clean['Humidity9am']
df_clean['CloudCover'] = df_clean['Cloud9am'] + df_clean['Cloud3pm']

# Convert RainToday and RainTomorrow to binary
df_clean['RainToday'] = df_clean['RainToday'].map({'Yes': 1, 'No': 0})
df_clean['RainTomorrow'] = df_clean['RainTomorrow'].map({'Yes': 1, 'No': 0})

# Drop unnecessary columns
columns_to_drop = ['Date', 'Location', 'Evaporation', 'Sunshine', 'WindGustDir', 
                   'WindDir9am', 'WindDir3pm', 'RISK_MM']
df_clean = df_clean.drop(columns=[col for col in columns_to_drop if col in df_clean.columns])

# Display new columns
print("New columns created:")
print(['TempRange', 'PressureChange', 'HumidityChange', 'CloudCover'])
print("\nDataset shape after feature engineering:", df_clean.shape)

# %% [markdown]
# ## Exercise 6: Encoding Categorical Variables
# Encode categorical variables for machine learning algorithms.

# %%
# Identify remaining categorical columns
categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()
print("Categorical columns to encode:", categorical_cols)

# Apply label encoding
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df_clean[col] = le.fit_transform(df_clean[col])
    label_encoders[col] = le
    print(f"{col}: {len(le.classes_)} classes")

# Verify encoding
print("\nFirst 5 rows after encoding:")
print(df_clean.head())

# %% [markdown]
# ## Exercise 7: Feature Scaling
# Scale the features to have zero mean and unit variance.

# %%
# Separate features and target
X = df_clean.drop('RainTomorrow', axis=1)
y = df_clean['RainTomorrow']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, 
                                                    random_state=42, stratify=y)

print("Training set shape:", X_train.shape)
print("Testing set shape:", X_test.shape)
print("\nClass distribution in training set:")
print(y_train.value_counts(normalize=True))

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert back to DataFrame for easier handling
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)

print("\nFeature scaling completed.")

# %% [markdown]
# ## Exercise 8: Building a Logistic Regression Model
# Create and train a logistic regression model.

# %%
# Initialize and train logistic regression model
log_reg = LogisticRegression(random_state=42, max_iter=1000)
log_reg.fit(X_train_scaled, y_train)

# Make predictions
y_pred_logreg = log_reg.predict(X_test_scaled)
y_pred_proba_logreg = log_reg.predict_proba(X_test_scaled)[:, 1]

# Evaluate the model
print("Logistic Regression Results:")
print("=" * 50)
print("Accuracy:", accuracy_score(y_test, y_pred_logreg))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_logreg))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_logreg))

# Calculate True Positive Rate (Recall for positive class)
tn, fp, fn, tp = confusion_matrix(y_test, y_pred_logreg).ravel()
tpr_logreg = tp / (tp + fn)
print(f"\nTrue Positive Rate (Recall): {tpr_logreg:.4f}")

# %% [markdown]
# ## Exercise 9: Building a Random Forest Classifier
# Create and train a random forest model.

# %%
# Initialize and train random forest model
rf_clf = RandomForestClassifier(random_state=42, n_estimators=100)
rf_clf.fit(X_train_scaled, y_train)

# Make predictions
y_pred_rf = rf_clf.predict(X_test_scaled)
y_pred_proba_rf = rf_clf.predict_proba(X_test_scaled)[:, 1]

# Evaluate the model
print("Random Forest Classifier Results:")
print("=" * 50)
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nConfusion Matrix:")
cm_rf = confusion_matrix(y_test, y_pred_rf)
print(cm_rf)
print("\nClassification Report:")
print(classification_report(y_test, y_pred_rf))

# Calculate True Positive Rate (Recall for positive class)
tn, fp, fn, tp = cm_rf.ravel()
tpr_rf = tp / (tp + fn)
print(f"\nTrue Positive Rate (Recall): {tpr_rf:.4f}")

# %% [markdown]
# ## Exercise 10: Model Comparison and Evaluation
# Compare the performance of both models.

# %%
# Create comparison DataFrame
comparison = pd.DataFrame({
    'Model': ['Logistic Regression', 'Random Forest'],
    'Accuracy': [
        accuracy_score(y_test, y_pred_logreg),
        accuracy_score(y_test, y_pred_rf)
    ],
    'True Positive Rate': [tpr_logreg, tpr_rf],
    'ROC AUC': [
        roc_auc_score(y_test, y_pred_proba_logreg),
        roc_auc_score(y_test, y_pred_proba_rf)
    ]
})

print("Model Comparison:")
print("=" * 50)
print(comparison.to_string(index=False))

# Visual comparison
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Accuracy comparison
axes[0].bar(comparison['Model'], comparison['Accuracy'], color=['skyblue', 'lightcoral'])
axes[0].set_title('Model Accuracy Comparison')
axes[0].set_ylabel('Accuracy')
axes[0].set_ylim([0, 1])
for i, v in enumerate(comparison['Accuracy']):
    axes[0].text(i, v + 0.01, f'{v:.3f}', ha='center')

# TPR comparison
axes[1].bar(comparison['Model'], comparison['True Positive Rate'], color=['skyblue', 'lightcoral'])
axes[1].set_title('True Positive Rate (Recall) Comparison')
axes[1].set_ylabel('True Positive Rate')
axes[1].set_ylim([0, 1])
for i, v in enumerate(comparison['True Positive Rate']):
    axes[1].text(i, v + 0.01, f'{v:.3f}', ha='center')

plt.tight_layout()
plt.show()

# %% [markdown]
# ## Exercise 11: Feature Importance Analysis
# Analyze which features are most important for prediction.

# %%
# Get feature importance from Random Forest
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_clf.feature_importances_
}).sort_values('importance', ascending=False)

print("Top 10 Most Important Features:")
print("=" * 50)
print(feature_importance.head(10).to_string(index=False))

# Plot feature importance
plt.figure(figsize=(12, 8))
bars = plt.barh(feature_importance.head(15)['feature'], 
                feature_importance.head(15)['importance'])
plt.xlabel('Feature Importance')
plt.title('Top 15 Feature Importances (Random Forest)')
plt.gca().invert_yaxis()

# Add value labels
for bar in bars:
    width = bar.get_width()
    plt.text(width + 0.001, bar.get_y() + bar.get_height()/2,
             f'{width:.3f}', ha='left', va='center')

plt.tight_layout()
plt.show()

# %% [markdown]
# ## Exercise 12: Cross-Validation
# Perform cross-validation to ensure model robustness.

# %%
# Perform cross-validation for both models
print("Cross-Validation Results (5-fold):")
print("=" * 50)

# Logistic Regression CV
logreg_cv_scores = cross_val_score(log_reg, X_train_scaled, y_train, 
                                   cv=5, scoring='accuracy')
print(f"Logistic Regression CV Accuracy: {logreg_cv_scores.mean():.4f} (+/- {logreg_cv_scores.std() * 2:.4f})")

# Random Forest CV
rf_cv_scores = cross_val_score(rf_clf, X_train_scaled, y_train, 
                               cv=5, scoring='accuracy')
print(f"Random Forest CV Accuracy: {rf_cv_scores.mean():.4f} (+/- {rf_cv_scores.std() * 2:.4f})")

# CV for ROC AUC
logreg_auc_scores = cross_val_score(log_reg, X_train_scaled, y_train, 
                                     cv=5, scoring='roc_auc')
rf_auc_scores = cross_val_score(rf_clf, X_train_scaled, y_train, 
                                 cv=5, scoring='roc_auc')

print(f"\nLogistic Regression CV ROC AUC: {logreg_auc_scores.mean():.4f} (+/- {logreg_auc_scores.std() * 2:.4f})")
print(f"Random Forest CV ROC AUC: {rf_auc_scores.mean():.4f} (+/- {rf_auc_scores.std() * 2:.4f})")

# %% [markdown]
# ## Exercise 13: Hyperparameter Tuning
# Tune hyperparameters for the Random Forest model.

# %%
# Define parameter grid for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Perform Grid Search
print("Performing Grid Search for Random Forest...")
rf_grid = GridSearchCV(RandomForestClassifier(random_state=42),
                       param_grid, 
                       cv=3, 
                       scoring='accuracy',
                       n_jobs=-1,
                       verbose=1)
rf_grid.fit(X_train_scaled, y_train)

print("\nBest Parameters:", rf_grid.best_params_)
print("Best CV Accuracy:", rf_grid.best_score_)

# Train model with best parameters
best_rf = rf_grid.best_estimator_
y_pred_best_rf = best_rf.predict(X_test_scaled)
print("\nTest Accuracy with Best Model:", accuracy_score(y_test, y_pred_best_rf))

# Confusion matrix for best model
cm_best_rf = confusion_matrix(y_test, y_pred_best_rf)
print("\nConfusion Matrix for Best Random Forest:")
print(cm_best_rf)

# Calculate True Positive Rate for best model
tn, fp, fn, tp = cm_best_rf.ravel()
tpr_best_rf = tp / (tp + fn)
print(f"\nTrue Positive Rate for Best Model: {tpr_best_rf:.4f}")

# %% [markdown]
# ## Exercise 14: Final Model Evaluation and Conclusions
# Provide final evaluation and conclusions.

# %%
# Final evaluation comparison
final_comparison = pd.DataFrame({
    'Model': ['Logistic Regression', 'Random Forest (Default)', 'Random Forest (Tuned)'],
    'Test Accuracy': [
        accuracy_score(y_test, y_pred_logreg),
        accuracy_score(y_test, y_pred_rf),
        accuracy_score(y_test, y_pred_best_rf)
    ],
    'True Positive Rate': [
        tpr_logreg,
        tpr_rf,
        tpr_best_rf
    ],
    'ROC AUC': [
        roc_auc_score(y_test, y_pred_proba_logreg),
        roc_auc_score(y_test, y_pred_proba_rf),
        roc_auc_score(y_test, best_rf.predict_proba(X_test_scaled)[:, 1])
    ]
})

print("FINAL MODEL COMPARISON:")
print("=" * 60)
print(final_comparison.to_string(index=False))

# Plot ROC curves
plt.figure(figsize=(10, 8))

# Logistic Regression ROC
fpr_logreg, tpr_logreg_roc, _ = roc_curve(y_test, y_pred_proba_logreg)
roc_auc_logreg = auc(fpr_logreg, tpr_logreg_roc)
plt.plot(fpr_logreg, tpr_logreg_roc, label=f'Logistic Regression (AUC = {roc_auc_logreg:.3f})')

# Random Forest Default ROC
fpr_rf, tpr_rf_roc, _ = roc_curve(y_test, y_pred_proba_rf)
roc_auc_rf = auc(fpr_rf, tpr_rf_roc)
plt.plot(fpr_rf, tpr_rf_roc, label=f'Random Forest Default (AUC = {roc_auc_rf:.3f})')

# Random Forest Tuned ROC
y_pred_proba_best_rf = best_rf.predict_proba(X_test_scaled)[:, 1]
fpr_best_rf, tpr_best_rf_roc, _ = roc_curve(y_test, y_pred_proba_best_rf)
roc_auc_best_rf = auc(fpr_best_rf, tpr_best_rf_roc)
plt.plot(fpr_best_rf, tpr_best_rf_roc, label=f'Random Forest Tuned (AUC = {roc_auc_best_rf:.3f})')

# Plot diagonal line
plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.show()

# Final conclusions
print("\n" + "=" * 60)
print("FINAL CONCLUSIONS:")
print("=" * 60)
print("\n1. Model Performance Summary:")
print(f"   - Logistic Regression Accuracy: {final_comparison.loc[0, 'Test Accuracy']:.3f}")
print(f"   - Random Forest (Default) Accuracy: {final_comparison.loc[1, 'Test Accuracy']:.3f}")
print(f"   - Random Forest (Tuned) Accuracy: {final_comparison.loc[2, 'Test Accuracy']:.3f}")

print("\n2. True Positive Rate (Recall for Rain Tomorrow):")
print(f"   - Logistic Regression TPR: {final_comparison.loc[0, 'True Positive Rate']:.3f}")
print(f"   - Random Forest (Default) TPR: {final_comparison.loc[1, 'True Positive Rate']:.3f}")
print(f"   - Random Forest (Tuned) TPR: {final_comparison.loc[2, 'True Positive Rate']:.3f}")

print("\n3. Best Model:")
best_model_idx = final_comparison['Test Accuracy'].idxmax()
best_model = final_comparison.loc[best_model_idx, 'Model']
print(f"   - {best_model} is the best performer with accuracy of {final_comparison.loc[best_model_idx, 'Test Accuracy']:.3f}")

print("\n4. Key Insights:")
print("   - Random Forest consistently outperforms Logistic Regression")
print("   - Hyperparameter tuning improved Random Forest performance")
print("   - The model is better at predicting 'No Rain' than 'Rain Tomorrow'")
print("   - Humidity, pressure, and cloud cover are key predictors")

print("\n5. Recommendations:")
print("   - Use the tuned Random Forest model for deployment")
print("   - Monitor model performance regularly")
print("   - Collect more data to improve minority class prediction")
print("   - Consider ensemble methods for further improvement")
